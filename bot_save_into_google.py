import os
import re
import gspread

import pandas as pd
from telegram import Update
from telegram.ext import ApplicationBuilder, CommandHandler, MessageHandler, filters, ContextTypes
from dotenv import load_dotenv
from oauth2client.service_account import ServiceAccountCredentials

from logger import get_logger

load_dotenv()

logger = get_logger()

TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_TOKEN")
GOOGLE_SHEET_KEY = os.getenv("GOOGLE_SHEET_KEY")
GOOGLE_SHEET = os.getenv("GOOGLE_SHEET")
CREDENTIALS_FILE = "credentials.json"
DATA_DIRECTORY = 'data'

# –°–ø–∏—Å–∫–∏ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
GENERIC_STOP_WORDS = {
    "—Å—Ç–∞–∂–∏—Ä–æ–≤–∫–∞", "–≤–∞–∫–∞–Ω—Å–∏—è", "–ø—Ä–∞–∫—Ç–∏–∫–∞", "–∫–∞—Ñ–µ–¥—Ä–∞", "—Ñ–∞–∫—É–ª—å—Ç–µ—Ç", "—Ü–µ–Ω—Ç—Ä", "–¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç", "—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ",
    "–≥–∫", "–æ–æ–æ", "–∑–∞–æ", "–ø–∞–æ", "–∞–æ", "ao", "pjsc", "llc", "inc", "corp", "co", "gmbh",
    "–∫–æ–º–ø–∞–Ω–∏—è", "—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç", "–∏–Ω—Å—Ç–∏—Ç—É—Ç", "–∫–æ–ª–ª–µ–¥–∂", "–∞–∫–∞–¥–µ–º–∏—è", "–ª–∞–±–æ—Ä–∞—Ç–æ—Ä–∏—è", "—à–∫–æ–ª–∞", "–æ–±—É—á–µ–Ω–∏–µ",
    "—Ä–∞–±–æ—Ç–∞", "–∫–∞—Ä—å–µ—Ä–∞", "–∫–æ–º–∞–Ω–¥–∞", "–ø—Ä–æ–µ–∫—Ç", "–ø—Ä–∏–≥–ª–∞—à–∞–µ—Ç", "–∏—â–µ—Ç", "–Ω–∞–±–æ—Ä"
}

COMPANY_LEGAL_FORMS = {"–æ–æ–æ", "–∞–æ", "–ø–∞–æ", "–∑–∞–æ", "ao", "pjsc", "llc", "inc", "co", "corp", "gmbh"}


def normalize_text(text: str) -> str:
    """
    –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Ç–µ–∫—Å—Ç: –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É, –∑–∞–º–µ–Ω—è–µ—Ç —ë –Ω–∞ –µ, —É–¥–∞–ª—è–µ—Ç –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ —Å–∏–º–≤–æ–ª—ã.
    Args: text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
    Returns: –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    """
    if pd.isna(text):
        return ""

    normalized = (str(text)
                  .replace("—ë", "–µ")
                  .strip()
                  .lower())
    normalized = re.sub(r"\s+", " ", normalized)
    normalized = normalized.strip("¬´¬ª\"'()[]")

    return normalized


def extract_company_mentions_from_text(text: str) -> list[str]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∫–æ–º–ø–∞–Ω–∏–π –∏–∑ —Ç–µ–∫—Å—Ç–∞, –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–≥–æ GPT.
    Args: text: –¢–µ–∫—Å—Ç —Å —É–ø–æ–º–∏–Ω–∞–Ω–∏—è–º–∏ –∫–æ–º–ø–∞–Ω–∏–π
    Returns: –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –∫–æ–º–ø–∞–Ω–∏–π
    """
    if pd.isna(text):
        return []

    # –£–¥–∞–ª—è–µ–º –ø—Ä–µ—Ñ–∏–∫—Å –ø–µ—Ä–µ–¥ –¥–≤–æ–µ—Ç–æ—á–∏–µ–º –µ—Å–ª–∏ –µ—Å—Ç—å
    text_after_colon = text.split(":", 1)[1] if ":" in text else text

    # –†–∞–∑–¥–µ–ª—è–µ–º —Ç–µ–∫—Å—Ç –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º
    separators = r"[;,‚Ä¢/|‚Äî\-‚Äì\n\.]"
    parts = re.split(separators, text_after_colon)

    mentions = []
    for part in parts:
        normalized_part = normalize_text(part)
        if normalized_part:
            mentions.append(normalized_part)

    return mentions


def is_valid_company_name(company_name: str) -> bool:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏ –≤–∞–ª–∏–¥–Ω—ã–º –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.
    Args: company_name: –ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
    Returns: True –µ—Å–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ –≤–∞–ª–∏–¥–Ω–æ, –∏–Ω–∞—á–µ False
    """
    if not company_name:
        return False

    # –ò—Å–∫–ª—é—á–∞–µ–º –æ–±—â–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
    if company_name in {"vk", "–≤–∫", "vk.com"}:
        return False

    # –ò—Å–∫–ª—é—á–∞–µ–º —á–∏—Å—Ç–æ —á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
    if company_name.isdigit():
        return False
    # –ò—Å–∫–ª—é—á–∞–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è
    if len(company_name) <= 2:
        return False

    # –ò—Å–∫–ª—é—á–∞–µ–º –æ–±—â–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
    if company_name in GENERIC_STOP_WORDS:
        return False

    # –£–¥–∞–ª—è–µ–º —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ —Ñ–æ—Ä–º—ã –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º –æ—Å—Ç–∞—Ç–æ–∫
    legal_forms_pattern = r"\b(" + "|".join(COMPANY_LEGAL_FORMS) + r")\b\.?"
    name_without_legal_form = re.sub(legal_forms_pattern, "", company_name).strip()

    return bool(name_without_legal_form)


def build_company_mappings(companies_dataframe: pd.DataFrame) -> tuple[dict, dict]:
    """
    –°–æ–∑–¥–∞–µ—Ç –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø—Å–µ–≤–¥–æ–Ω–∏–º–æ–≤ –∫–æ–º–ø–∞–Ω–∏–π –Ω–∞ –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∏ –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏—Ö –Ω–∞–∑–≤–∞–Ω–∏–π –Ω–∞ –¥–∞–Ω–Ω—ã–µ CRM.
    Args: companies_dataframe: DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏ –æ –∫–æ–º–ø–∞–Ω–∏—è—Ö
    Returns: –ö–æ—Ä—Ç–µ–∂ (alias_to_canonical, canonical_to_crm_data)
    """
    logger.info("–ù–∞—á–∞–ª–æ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –º–∞–ø–ø–∏–Ω–≥–æ–≤ –∫–æ–º–ø–∞–Ω–∏–π")
    
    alias_to_canonical = {}
    canonical_to_crm_data = {}

    for index, company_row in companies_dataframe.iterrows():
        canonical_name = normalize_text(company_row.get("–ü–æ–ª–Ω–æ–µ –∏–º—è", ""))
        if not canonical_name:
            logger.debug(f"–ü—Ä–æ–ø—É—Å–∫ —Å—Ç—Ä–æ–∫–∏ {index}: –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ")
            continue

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ CRM –¥–ª—è –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–æ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏—è
        canonical_to_crm_data[canonical_name] = company_row.to_dict()
        alias_to_canonical[canonical_name] = canonical_name

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø—Å–µ–≤–¥–æ–Ω–∏–º—ã (Also Known As)
        aka_names = company_row.get("Also known as (AKA)", "")
        if not pd.isna(aka_names):
            for alias in str(aka_names).split(","):
                normalized_alias = normalize_text(alias)
                if normalized_alias:
                    alias_to_canonical[normalized_alias] = canonical_name

    # –û—á–∏—â–∞–µ–º –º–∞–ø–ø–∏–Ω–≥ –æ—Ç –Ω–µ–≤–∞–ª–∏–¥–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
    valid_two_letter_names = {name for name in canonical_to_crm_data.keys() if len(name) == 2}
    invalid_aliases = {",", ".", "-", "‚Äì", "‚Äî", "/", "|", "vk", "–≤–∫"}

    for alias in list(alias_to_canonical.keys()):
        if (alias in invalid_aliases or
            (len(alias) <= 2 and alias not in valid_two_letter_names)):
            alias_to_canonical.pop(alias, None)

    logger.info(f"–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–∞–ø–ø–∏–Ω–≥–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: {len(canonical_to_crm_data)} –∫–æ–º–ø–∞–Ω–∏–π, {len(alias_to_canonical)} –∞–ª–∏–∞—Å–æ–≤")
    return alias_to_canonical, canonical_to_crm_data


def find_company_mentions_in_post(post_gpt_text: str, alias_to_canonical_mapping: dict) -> set[str]:
    """
    –ù–∞—Ö–æ–¥–∏—Ç —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∫–æ–º–ø–∞–Ω–∏–π –≤ —Ç–µ–∫—Å—Ç–µ –ø–æ—Å—Ç–∞.
    Args: post_gpt_text: –¢–µ–∫—Å—Ç –ø–æ—Å—Ç–∞, –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π GPT
        alias_to_canonical_mapping: –ú–∞–ø–ø–∏–Ω–≥ –ø—Å–µ–≤–¥–æ–Ω–∏–º–æ–≤ –Ω–∞ –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è
    Returns: –ú–Ω–æ–∂–µ—Å—Ç–≤–æ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∫–æ–º–ø–∞–Ω–∏–π (–∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∏ –≤–∞–ª–∏–¥–Ω—ã–µ —Å–≤–æ–±–æ–¥–Ω—ã–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è)
    """
    mentioned_companies = set()
    extracted_mentions = extract_company_mentions_from_text(post_gpt_text)

    logger.debug(f"–ò–∑–≤–ª–µ—á–µ–Ω–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –∏–∑ —Ç–µ–∫—Å—Ç–∞: {len(extracted_mentions)}")

    for mention in extracted_mentions:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä—è–º–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –≤ –º–∞–ø–ø–∏–Ω–≥–µ
        if mention in alias_to_canonical_mapping:
            mentioned_companies.add(alias_to_canonical_mapping[mention])
            continue

        # –ü—Ä–æ–±—É–µ–º —É–¥–∞–ª–∏—Ç—å —é—Ä–∏–¥–∏—á–µ—Å–∫—É—é —Ñ–æ—Ä–º—É –∏ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Å–Ω–æ–≤–∞
        legal_forms_pattern = r"\b(" + "|".join(COMPANY_LEGAL_FORMS) + r")\b\.?"
        mention_without_legal_form = re.sub(legal_forms_pattern, "", mention).strip()

        if (mention_without_legal_form and 
            mention_without_legal_form in alias_to_canonical_mapping):
            mentioned_companies.add(alias_to_canonical_mapping[mention_without_legal_form])
            continue

        # –î–æ–±–∞–≤–ª—è–µ–º –≤–∞–ª–∏–¥–Ω—ã–µ —Å–≤–æ–±–æ–¥–Ω—ã–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è
        if is_valid_company_name(mention):
            mentioned_companies.add(mention)

    logger.debug(f"–ù–∞–π–¥–µ–Ω–æ –∫–æ–º–ø–∞–Ω–∏–π –≤ –ø–æ—Å—Ç–µ: {len(mentioned_companies)}")
    return mentioned_companies


def process_uploaded_file(file_path: str) -> pd.DataFrame:
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π —Ñ–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏ –æ –ø–æ—Å—Ç–∞—Ö –∏ –∫–æ–º–ø–∞–Ω–∏—è—Ö.
    Args: file_path: –ü—É—Ç—å –∫ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É
    Returns: DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
    """
    logger.info(f"–ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞: {file_path}")
    
    try:
        excel_data = pd.ExcelFile(file_path)
        posts_dataframe = pd.read_excel(excel_data, sheet_name="vk")
        companies_dataframe = pd.read_excel(excel_data, sheet_name="–¥–ª—è –í–ü–†")
        
        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ –ø–æ—Å—Ç–æ–≤: {len(posts_dataframe)}, –∫–æ–º–ø–∞–Ω–∏–π: {len(companies_dataframe)}")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞ {file_path}: {str(e)}")
        raise

    # –°—Ç—Ä–æ–∏–º –º–∞–ø–ø–∏–Ω–≥–∏ –∫–æ–º–ø–∞–Ω–∏–π
    alias_to_canonical, canonical_to_crm = build_company_mappings(companies_dataframe)
    canonical_company_names = set(canonical_to_crm.keys())

    # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —É–ø–æ–º–∏–Ω–∞–Ω–∏–π
    company_mentions = {}
    processed_posts = 0

    for index, post_row in posts_dataframe.iterrows():
        # –ü–æ–ª—É—á–∞–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ –ø–æ—Å—Ç
        post_link = post_row.get("–ü–æ—Å—Ç")
        if pd.isna(post_link) or not str(post_link).strip():
            post_link = post_row.get("–ì—Ä—É–ø–ø–∞", "")

        # –ù–∞—Ö–æ–¥–∏–º –∫–æ–º–ø–∞–Ω–∏–∏, —É–ø–æ–º—è–Ω—É—Ç—ã–µ –≤ –ø–æ—Å—Ç–µ
        gpt_text = post_row.get("GPT", "")
        companies_in_post = find_company_mentions_in_post(gpt_text, alias_to_canonical)

        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–ª—è –∫–∞–∂–¥–æ–π –Ω–∞–π–¥–µ–Ω–Ω–æ–π –∫–æ–º–ø–∞–Ω–∏–∏
        for company in companies_in_post:
            crm_data = canonical_to_crm.get(company) if company in canonical_company_names else None

            if company not in company_mentions:
                company_mentions[company] = {
                    "mention_count": 0,
                    "post_links": [],
                    "crm_data": crm_data
                }

            company_mentions[company]["mention_count"] += 1

            if post_link and str(post_link) not in company_mentions[company]["post_links"]:
                company_mentions[company]["post_links"].append(str(post_link))
        
        processed_posts += 1
        if processed_posts % 100 == 0:
            logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –ø–æ—Å—Ç–æ–≤: {processed_posts}/{len(posts_dataframe)}")

    logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ—Å—Ç–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –£–ø–æ–º–∏–Ω–∞–Ω–∏–π –Ω–∞–π–¥–µ–Ω–æ: {len(company_mentions)}")

    # –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç
    report_rows = []
    for company, mention_data in company_mentions.items():
        crm_data = mention_data["crm_data"]

        report_rows.append({
            "#": crm_data.get("#") if crm_data else "",
            "–ö–æ–º–ø–∞–Ω–∏—è": company,
            "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π": mention_data["mention_count"],
            "–°—Å—ã–ª–∫–∏ –Ω–∞ –ø–æ—Å—Ç—ã": ", ".join(mention_data["post_links"]),
            "–û—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω—ã–π –ò–≤–µ–Ω—Ç—ã": crm_data.get("–û—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω—ã–π –î–ö") if crm_data else "",
            "–û—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω—ã–π –ú–µ–¥–∏–∞": crm_data.get("–û—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω—ã–π Media") if crm_data else "",
            "–ï—Å—Ç—å –≤ –°–†–ú": "–î–∞" if company in canonical_company_names else "–ù–µ—Ç",
        })

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏—Ç–æ–≥–æ–≤–æ–≥–æ DataFrame
    report_columns = [
        "#", "–ö–æ–º–ø–∞–Ω–∏—è", "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π", "–°—Å—ã–ª–∫–∏ –Ω–∞ –ø–æ—Å—Ç—ã",
        "–û—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω—ã–π –ò–≤–µ–Ω—Ç—ã", "–û—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω—ã–π –ú–µ–¥–∏–∞", "–ï—Å—Ç—å –≤ –°–†–ú"
    ]

    report_dataframe = pd.DataFrame(report_rows, columns=report_columns)
    sorted_report = report_dataframe.sort_values(by="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π", ascending=False)

    logger.info(f"–§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: {len(report_rows)} –∑–∞–ø–∏—Å–µ–π")
    return sorted_report.reset_index(drop=True)


def get_google_sheet_client():
    """–°–æ–∑–¥–∞–µ—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–ª–∏–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Google Sheets"""
    try:
        scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
        creds = ServiceAccountCredentials.from_json_keyfile_name(CREDENTIALS_FILE, scope)
        client = gspread.authorize(creds)
        logger.info("–£—Å–ø–µ—à–Ω–æ–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Google Sheets")
        return client
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Google Sheets: {str(e)}")
        raise


def save_to_google_sheets(dataframe: pd.DataFrame, worksheet_name: str = "–û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ") -> str:
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç DataFrame –≤ Google –¢–∞–±–ª–∏—Ü—É –Ω–∞ —É–∫–∞–∑–∞–Ω–Ω—ã–π –ª–∏—Å—Ç
    Returns: –°—Å—ã–ª–∫–∞ –Ω–∞ —Ç–∞–±–ª–∏—Ü—É
    """
    logger.info(f"–ù–∞—á–∞–ª–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –≤ Google Sheets. –ó–∞–ø–∏—Å–µ–π: {len(dataframe)}")
    
    try:
        client = get_google_sheet_client()
        spreadsheet = client.open_by_key(GOOGLE_SHEET_KEY)

        try:
            # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –ª–∏—Å—Ç
            worksheet = spreadsheet.worksheet(worksheet_name)
            logger.info(f"–õ–∏—Å—Ç '{worksheet_name}' –Ω–∞–π–¥–µ–Ω, –æ—á–∏—â–∞–µ–º –∏ –æ–±–Ω–æ–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ...")
        except gspread.WorksheetNotFound:
            # –ï—Å–ª–∏ –ª–∏—Å—Ç –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π
            logger.info(f"–õ–∏—Å—Ç '{worksheet_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π...")
            worksheet = spreadsheet.add_worksheet(title=worksheet_name, rows="1000", cols="20")

        # –û—á–∏—â–∞–µ–º –ª–∏—Å—Ç
        worksheet.clear()
        logger.debug("–õ–∏—Å—Ç –æ—á–∏—â–µ–Ω")

        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
        data_to_upload = [dataframe.columns.tolist()]  # –ó–∞–≥–æ–ª–æ–≤–∫–∏
        data_to_upload.extend(dataframe.fillna('').values.tolist())  # –î–∞–Ω–Ω—ã–µ

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –æ–¥–Ω–æ–π –æ–ø–µ—Ä–∞—Ü–∏–µ–π (–±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ)
        worksheet.update(data_to_upload, 'A1')
        logger.debug("–î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ —Ç–∞–±–ª–∏—Ü—É")

        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏
        worksheet.format('A1:Z1', {
            'textFormat': {'bold': True},
            'backgroundColor': {'red': 0.9, 'green': 0.9, 'blue': 0.9}
        })
        logger.debug("–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –ø—Ä–∏–º–µ–Ω–µ–Ω–æ")

        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ–¥–±–∏—Ä–∞–µ–º —à–∏—Ä–∏–Ω—É –∫–æ–ª–æ–Ω–æ–∫
        try:
            worksheet.columns_auto_resize(0, len(dataframe.columns))
            logger.debug("–ê–≤—Ç–æ–ø–æ–¥–±–æ—Ä —à–∏—Ä–∏–Ω—ã –∫–æ–ª–æ–Ω–æ–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω")
        except Exception as e:
            logger.warning(f"–ê–≤—Ç–æ–ø–æ–¥–±–æ—Ä —à–∏—Ä–∏–Ω—ã –∫–æ–ª–æ–Ω–æ–∫ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è: {str(e)}")

        logger.info(f"–î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –Ω–∞ –ª–∏—Å—Ç '{worksheet_name}'")
        return f"https://docs.google.com/spreadsheets/d/{GOOGLE_SHEET_KEY}/edit#gid={worksheet.id}"

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤ Google Sheets: {str(e)}")
        raise


async def start_command(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∫–æ–º–∞–Ω–¥—É /start"""
    logger.info(f"–ö–æ–º–∞–Ω–¥–∞ /start –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {update.effective_user.id}")
    welcome_message = (
        "–ü—Ä–∏–≤–µ—Ç! –û—Ç–ø—Ä–∞–≤—å—Ç–µ —Ñ–∞–π–ª —Å –ø–æ—Å—Ç–∞–º–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏. "
        "–†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ –Ω–µ –¥–æ–ª–∂–µ–Ω –ø—Ä–µ–≤—ã—à–∞—Ç—å 20 –ú–ë"
    )
    await update.message.reply_text(welcome_message)


async def handle_file_upload(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã"""
    user_id = update.effective_user.id
    uploaded_file = update.message.document
    
    logger.info(f"–ü–æ–ª—É—á–µ–Ω —Ñ–∞–π–ª –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}: {uploaded_file.file_name}")
    
    file_object = await uploaded_file.get_file()
    file_path = os.path.join(DATA_DIRECTORY, uploaded_file.file_name)

    try:
        # –°–∫–∞—á–∏–≤–∞–µ–º —Ñ–∞–π–ª
        await file_object.download_to_drive(custom_path=file_path)
        logger.info(f"–§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {file_path}")
        
        await update.message.reply_text(f"–§–∞–π–ª {uploaded_file.file_name} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω. –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é...")

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª
        processed_data = process_uploaded_file(file_path)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Google –¢–∞–±–ª–∏—Ü—É –Ω–∞ –ª–∏—Å—Ç "–û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ"
        sheet_url = save_to_google_sheets(processed_data, "–û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")

        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –∏ —Å—Å—ã–ª–∫—É –Ω–∞ —Ç–∞–±–ª–∏—Ü—É
        success_message = (
            f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!\n"
            f"üìä –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ Google –¢–∞–±–ª–∏—Ü—É –Ω–∞ –ª–∏—Å—Ç '–û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ':\n"
            f"{sheet_url}"
        )

        await update.message.reply_text(success_message)
        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}")

    except Exception as error:
        error_message = f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞: {str(error)}"
        logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}: {str(error)}", exc_info=True)
        await update.message.reply_text(error_message)


def setup_bot_handlers(application) -> None:
    """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ –∫–æ–º–∞–Ω–¥ –∏ —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –±–æ—Ç–∞"""
    application.add_handler(CommandHandler("start", start_command))
    application.add_handler(MessageHandler(filters.Document.ALL, handle_file_upload))
    logger.info("–û–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ –±–æ—Ç–∞ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã")


def main() -> None:
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞ –±–æ—Ç–∞"""
    logger.info("–ó–∞–ø—É—Å–∫ –±–æ—Ç–∞...")
    
    # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è –¥–∞–Ω–Ω—ã—Ö –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    if not os.path.exists(DATA_DIRECTORY):
        os.makedirs(DATA_DIRECTORY)
        logger.info(f"–°–æ–∑–¥–∞–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {DATA_DIRECTORY}")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
    if not TELEGRAM_BOT_TOKEN:
        logger.error("–ù–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –æ–∫—Ä—É–∂–µ–Ω–∏—è TELEGRAM_TOKEN")
        raise ValueError("TELEGRAM_TOKEN –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
    
    if not GOOGLE_SHEET_KEY:
        logger.error("–ù–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –æ–∫—Ä—É–∂–µ–Ω–∏—è GOOGLE_SHEET_KEY")
        raise ValueError("GOOGLE_SHEET_KEY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")

    try:
        # –°–æ–∑–¥–∞–µ–º –∏ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –±–æ—Ç–∞
        bot_application = ApplicationBuilder().token(TELEGRAM_BOT_TOKEN).build()
        setup_bot_handlers(bot_application)

        # –ó–∞–ø—É—Å–∫–∞–µ–º –±–æ—Ç–∞
        logger.info("–ë–æ—Ç –∑–∞–ø—É—â–µ–Ω –∏ –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ")
        bot_application.run_polling()
        
    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ –±–æ—Ç–∞: {str(e)}", exc_info=True)
        raise


if __name__ == '__main__':
    main()